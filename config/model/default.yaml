# config/model/default.yaml
pretrained_type: "bert_only"  # Options: "none", "bert_only", "full", "single-cell", "single-cell-from-scratch"

# Path of model to load or BERT component (depending on pretrained_type)
bert_path_or_name: "/grid/zador/data_norepl/Ari/transcriptomics/geneformer_models/retraining_1707495054775589/models"

# Model architecture parameters
num_labels: 290
num_set_layers: 4
set_hidden_size: 768
num_attention_heads: 8
dropout_prob: 0.1

# Model behavior flags
detach_bert_embeddings: false  # Whether to detach BERT embeddings
single_cell_loss_after_set: false  # Whether to apply single-cell loss after set transformer
single_cell_vs_group_weight: 0.5

# Position encoding parameters
relative_positions:
  enabled: false
  absolute_Z: false
  encoding_dim: 48  # Must ensure (set_hidden_size + position_encoding_dim) is divisible by num_attention_heads
  encoding_type: "mlp"  # Options: "mlp" or "sinusoidal". position_encoding_dim must be divisble by 6 for sinusoidal.

# Optional BERT config parameters (used when pretrained_type is "none")
bert_params:
  vocab_size: 112
  hidden_size: 512
  num_hidden_layers: 12
  num_attention_heads_bert: 8
  intermediate_size: 1024
  max_position_embeddings: 106
  attention_probs_dropout_prob: 0.02
  initializer_range: 0.02
  pad_token_id: 0
  position_embedding_type: absolute
  type_vocab_size: 2
  use_cache: true
  torch_dtype: "float32"
  classifier_dropout: null
