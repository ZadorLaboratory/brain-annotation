# config/model/default.yaml
pretrained_type: "bert_only"  # Options: "none", "bert_only", "full", "single-cell"

# Path or name for BERT component (used when pretrained_type is "bert_only")
bert_path_or_name: "/home/benjami/mnt/zador_data_norepl/Ari/transcriptomics/geneformer_models/retraining_1707495054775589/models"  # barseq_pretrained_cls

# Path for full model (used when pretrained_type is "full")
model_path: null  

num_labels: 290
num_set_layers: 4
set_hidden_size: 768
num_attention_heads: 8
dropout_prob: 0.1 # dropout on cells in group pooling
pool_weight: 0.5 # 'learned' or float 0-1. 1 is a model average of single-cell logits, 0 is all set transformer.
detach_bert_embeddings: false
detach_single_cell_logits: true
also_single_cell_loss: false

# Optional BERT config parameters (used when pretrained_type is "none")
bert_params:
  vocab_size: 112
  hidden_size: 512
  num_hidden_layers: 12
  num_attention_heads_bert: 8
  intermediate_size: 1024
  max_position_embeddings: 106
  attention_probs_dropout_prob: 0.02
  initializer_range: 0.02
  pad_token_id: 0
  position_embedding_type: absolute
  type_vocab_size: 2
  use_cache: true
  torch_dtype: "float32"
  classifier_dropout: null
