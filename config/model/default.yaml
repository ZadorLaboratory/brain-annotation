# config/model/default.yaml
pretrained_type: "bert_only"  # Options: "none", "bert_only", "full", "single-cell", "single-cell-from-scratch"

# Path or name for BERT component (used when pretrained_type is "bert_only")
bert_path_or_name: "/grid/zador/data_norepl/Ari/transcriptomics/Ari/transcriptomics/geneformer_models/retraining_1707495054775589/models"

# Path for full model (used when pretrained_type is "full")
model_path: null

# Model architecture parameters
num_labels: 290
num_set_layers: 4
set_hidden_size: 768
num_attention_heads: 8
dropout_prob: 0.1

# Model behavior flags
pool_weight: 0.5  # Weight for single-cell predictions (0-1 or "learned")
single_cell_augmentation: false  # Whether to use additional single-cell classification
detach_bert_embeddings: false  # Whether to detach BERT embeddings
detach_single_cell_logits: true  # Whether to detach single-cell logits
single_cell_loss_after_set: false  # Whether to apply single-cell loss after set transformer

# Position encoding parameters
use_relative_positions: false  # Whether to use relative position encoding
position_encoding_dim: 32  # Must ensure (set_hidden_size + position_encoding_dim) is divisible by num_attention_heads

# Optional BERT config parameters (used when pretrained_type is "none")
bert_params:
  vocab_size: 112
  hidden_size: 512
  num_hidden_layers: 12
  num_attention_heads_bert: 8
  intermediate_size: 1024
  max_position_embeddings: 106
  attention_probs_dropout_prob: 0.02
  initializer_range: 0.02
  pad_token_id: 0
  position_embedding_type: absolute
  type_vocab_size: 2
  use_cache: true
  torch_dtype: "float32"
  classifier_dropout: null
